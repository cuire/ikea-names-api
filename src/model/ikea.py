# -*- coding: utf-8 -*-
"""
Automatically generated by Colaboratory.

[RNN](https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/)
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

from .const import CHAR_TO_INDEX, VOCAB_SIZE

device = "cuda" if torch.cuda.is_available() else "cpu"


RNN_SIZE = 100


class RNN(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.rnn = nn.RNN(input_size=VOCAB_SIZE, hidden_size=RNN_SIZE, batch_first=True)
        self.fc = nn.Linear(RNN_SIZE, VOCAB_SIZE)

    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        batch_size = x.size(0)
        out, hidden = self.rnn(x)
        out = out.contiguous().view(-1, RNN_SIZE)
        out = self.fc(out)

        return out, hidden


def get_traindata() -> tuple[torch.Tensor, torch.Tensor]:
    with open("./data/ikea_names.txt", "r") as f:
        data = f.read()

    data = data.lower()
    examples = data.split("\n")

    data_size, max_len = (
        len(examples),
        len(max(examples, key=len)),
    )

    X = torch.zeros(size=(data_size, max_len, VOCAB_SIZE))
    Y = torch.zeros(size=(data_size, max_len), dtype=torch.long)

    # Make items same size
    for i in range(len(examples)):
        while len(examples[i]) < max_len:
            examples[i] += " "

    for example_idx, example in enumerate(examples):
        text_x = example[:-1]
        for i, char in enumerate(text_x):
            X[example_idx][i][CHAR_TO_INDEX[char]] = 1

        text_y = example[1:]
        for i, char in enumerate(text_y):
            Y[example_idx][i] = CHAR_TO_INDEX[char]

    return X, Y


def train(
    model: RNN,
    traindata: torch.Tensor,
    expected: torch.Tensor,
    epochs: int,
):
    train_losses = np.zeros(epochs)

    optimizer = optim.Adam(model.parameters(), lr=0.01)
    lossfunction = nn.CrossEntropyLoss()

    for i in range(epochs):
        model.train()
        optimizer.zero_grad()
        output, hidden = model(traindata)
        loss = lossfunction(output, expected.view(-1).long())
        loss.backward()
        optimizer.step()
        train_losses[i] = loss.item()

        if (i + 1) % 100 == 0:
            print(f"Epoch:{i+1}/{epochs}, Train Loss: {loss:.4f}")

    return train_losses


def main() -> None:
    X, Y = get_traindata()

    model = RNN()
    model.to(device)

    _train_losses = train(
        model,
        X,
        Y,
        5000,
    )

    model_scripted = torch.jit.script(model)
    model_scripted.save("model_scripted.pt")


if __name__ == "__main__":
    main()
